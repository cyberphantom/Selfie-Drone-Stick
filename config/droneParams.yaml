# Environment Parameters
input_frame_size:
  w: 640
  h: 360

max_incl: 8.0 # Max space width in meters

alt_TH:
  max: 5.0   # in meters
  min: 0.5

yaw_TH:
  min: -1.55
  max: 1.55

ratio_TH:
  min: 0.00
  max: 0.24

init_state:
  px: 0.0
  py: 0.0
  pz: 0.0
  ox: 0.0
  oy: 0.0
  oz: 0.0
  ow: 0.0

goal:
    yaw: 0.0
    cx: 320.0
    cy: 180.0
    ratio: 0.1


# Training Algortihm Parameters
training: False
resume: True
resume_ep: 19500
output_graph: True
lr_A: 0.0002
lr_C: 0.0007
gamma: 0.95 # Higher gamma = smaller discount means care more about long term reward
            # Small gamma = High discount means care more about short term reward.
alpha: 0.1
epsilon_start: 1.0
epsilon_stop: 0.05
explore_start: 0.01 # 1.0
explore_stop: 0.01
d_rate: 0.00005 # TODO: change 0.00005
decay_rate: 0.99
batch_size: 256 # TODO: enlarge
tauTH: 50
learning_rate: 2e-4
memory_size: 256


# Reward parameters
nactions: 9  # Discrete action Space
speed: 0.3    # in m/s
stepsTH: 20 # Each step = 0.25ms => 5 sec
episodesTH: 2560000
step_duration: 0.5   # in seconds
rospy_rate: 50
stack_size: 4
ob_size: 4

obs_trails: 2.0
failed_detect_TH: 6
failed_sec_detect_TH: 4
